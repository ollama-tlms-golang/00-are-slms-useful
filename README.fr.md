# Est-ce que ces petits LLMs sont vraiment utiles ?

Pour ce premier article de l'ann√©e 2025, je vais faire court et r√©pondre √† une question que l'on me pose souvent : "Est-ce que ces petits LLMs sont vraiment utiles ?"

La r√©ponse courte est **oui**, bien s√ªr. 

Et je vais vous donner quelques exemples de mon utilisations personnelle de ces "tiny" mod√®les.

## Mon petit bout de code pour utiliser les "tiny" mod√®les

J'utilise tous les jours (et plusieurs fois par jour) ce bout de code (en Go) pour tester des LLMs avec Ollama, mais aussi pour m'aider √† g√©n√©rer des contenus divers et vari√©s :

```golang
package main

import (
	"context"
	"fmt"
	"log"
	"net/http"
	"net/url"
	"os"

	"github.com/ollama/ollama/api"
)

var (
	FALSE = false
	TRUE  = true
)

func main() {
	ctx := context.Background()

	var ollamaRawUrl string
	if ollamaRawUrl = os.Getenv("OLLAMA_HOST"); ollamaRawUrl == "" {
		ollamaRawUrl = "http://localhost:11434"
	}

	model := "qwen2.5-coder:1.5b"
	systemInstructions := "You are a useful AI agent."

	url, _ := url.Parse(ollamaRawUrl)

	client := api.NewClient(url, http.DefaultClient)

	// Load the content of the prompt.md file
	prompt, err := os.ReadFile("prompt.md")
	if err != nil {
		log.Fatalln("üò°", err)
	}

	// Prompt construction
	messages := []api.Message{
		{Role: "system", Content: systemInstructions},
		{Role: "user", Content: string(prompt)},
	}

	req := &api.ChatRequest{
		Model:    model,
		Messages: messages,
		Options: map[string]interface{}{
			"temperature":   0.0,
			"repeat_last_n": 2,
		},
		Stream: &TRUE,
	}

	answer := ""
	err = client.Chat(ctx, req, func(resp api.ChatResponse) error {
		answer += resp.Message.Content
		fmt.Print(resp.Message.Content)
		return nil
	})

	if err != nil {
		log.Fatalln("üò°", err)
	}

	// generate a markdown file from the value of answer
	err = os.WriteFile("report.md", []byte(answer), 0644)
	if err != nil {
		log.Fatalln("üò°", err)
	}
}
```

### Mais que fait ce code ?

Ce programme Go utilise et profite de la puissance de l'API d'Ollama pour g√©n√©rer du contenu √† partir d'un prompt envoy√© √† un LLM local.

#### √âtapes principales

1. **Configuration**
   - Connection √† Ollama (par d√©faut: localhost:11434)
   - Utilisation du mod√®le Qwen 2.5 Coder (dans cet exemple, mais je change souvent de mod√®le)

2. **Traitement**
   - Lecture du fichier `prompt.md`, c'est dans ce fichier que je vais poser ma question.
   - Construction du message avec les instructions syst√®me (dans cet exemple, "You are a useful AI agent.")
   - Envoi √† l'API avec diverses options (ici, par exemple temp√©rature √† `0.0`)

3. **Sortie**
   - Affichage en temps r√©el de la r√©ponse
   - Sauvegarde de la r√©ponse dans le fichier `report.md`

## Exemples d'utilisation

Je me sers de ce bout de code pour g√©n√©rer des contenus divers et vari√©s, comme par exemple :

### Des diagrammes √† partir d'une simple explication

J'ai utilis√© ce LLM : [qwen2.5-coder:1.5b](https://ollama.com/library/qwen2.5-coder:1.5b) pour g√©n√©rer un diagramme √† partir d'une simple explication.

#### Diagramme
![mermaid](imgs/01-mermaid-diag.png)

#### Le prompt associ√©

![mermaid](imgs/02-mermaid-prompt.png)

###  Des explications de code source avec des diagrammes associ√©s

J'ai utilis√© ce LLM : [qwen2.5-coder:14b](https://ollama.com/library/qwen2.5-coder:14b) pour g√©n√©rer une explication de code source et des diagrammes.

#### R√©sultat

![explain](imgs/03-explain.png)

![explain](imgs/04-explain.png)

![explain](imgs/05-explain.png)


### Un tutorial Rust pour d√©butants

J'ai utilis√© ce LLM : [qwen2.5-coder:14b](https://ollama.com/library/qwen2.5-coder:14b) pour g√©n√©rer ce tutorial sur Rust.

#### R√©sultat

![rust](imgs/06-rust.png)

![rust](imgs/07-rust.png)

#### Le prompt associ√©

![rust](imgs/08-rust.png)

Vous voyez, c'est vraiment utile ! Cela fonctionne en local sur ma machine sans Claude.ai, Gemini ou ChatGPT. Je l'utilise m√™me pour g√©n√©rer des sc√©narios de jeu de r√¥le, des recettes de cuisine, etc.

Et dernier point, mais non des moindres, je me suis d√©cid√© pour ce premier jour de 2025 √† faire de ce bout de code une CLI pour que je puisse l'utiliser encore plus facilement. C'est donc mon premier side project de l'ann√©e que vous pouvez retrouver ici : [https://github.com/sea-monkeys/bob](https://github.com/sea-monkeys/bob).

Amusez vous bien et √† tr√®s bient√¥t pour de nouvelles aventures avec les LLMs !




